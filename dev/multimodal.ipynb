{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholasking/code/ms/guidance/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "from guidance import image, user, assistant, models, gen\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/triangle1.png\n"
     ]
    }
   ],
   "source": [
    "triangle_img_path = os.path.join(os.path.curdir, 'images', 'triangle1.png')\n",
    "print(triangle_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - had to update `pip install -U google-ai-generativelanguage` to get this to work\n",
    "gemini = models.GoogleAI(\"models/gemini-1.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Hello</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>Hello</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>!</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> How</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> can</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> I</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> help</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> you</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> today</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>?</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>\n",
       "</span></pre></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with user():\n",
    "    lm = gemini + \"Hello\"\n",
    "\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Hello</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>Hello</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>!</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> How</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> can</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> I</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> help</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> you</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> today</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>?</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>\n",
       "</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2);  justify-content: center; align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>What is this? &lt;|_image:12964234752|&gt;</pre></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'GoogleAIChatEngine' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     lm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is this? \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m image(triangle_img_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m assistant():\n\u001b[0;32m----> 5\u001b[0m     lm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gen()\n",
      "File \u001b[0;32m~/code/ms/guidance/guidance/models/_model.py:1102\u001b[0m, in \u001b[0;36mModel.__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# run stateless functions (grammar nodes)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, GrammarFunction):\n\u001b[0;32m-> 1102\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stateless\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# run stateful functions\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m     out \u001b[38;5;241m=\u001b[39m value(lm)\n",
      "File \u001b[0;32m~/code/ms/guidance/guidance/models/_model.py:1307\u001b[0m, in \u001b[0;36mModel._run_stateless\u001b[0;34m(self, stateless_function, temperature, top_p, n)\u001b[0m\n\u001b[1;32m   1305\u001b[0m delayed_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# last_is_generated = False\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen_obj:\n\u001b[1;32m   1308\u001b[0m \n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;66;03m# we make everything full probability if we are not computing uncertainty\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;66;03m# if not self.engine.compute_log_probs:\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m#     chunk.new_bytes_prob = 1.0\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;66;03m# convert the bytes to a string (delaying if we don't yet have a valid unicode string)\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m     lm\u001b[38;5;241m.\u001b[39mtoken_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mnew_token_count\n\u001b[1;32m   1315\u001b[0m     chunk\u001b[38;5;241m.\u001b[39mnew_bytes \u001b[38;5;241m=\u001b[39m delayed_bytes \u001b[38;5;241m+\u001b[39m chunk\u001b[38;5;241m.\u001b[39mnew_bytes\n",
      "File \u001b[0;32m~/code/ms/guidance/guidance/models/_model.py:698\u001b[0m, in \u001b[0;36mEngine.__call__\u001b[0;34m(self, parser, grammar, ensure_bos_token)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    697\u001b[0m     token_ids, forced_bytes, current_temp \u001b[38;5;241m=\u001b[39m logits_state\n\u001b[0;32m--> 698\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_done:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/ms/guidance/guidance/models/_grammarless.py:373\u001b[0m, in \u001b[0;36mGrammarlessEngine.get_logits\u001b[0;34m(self, token_ids, forced_bytes, current_temp)\u001b[0m\n\u001b[1;32m    371\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_bytes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from _data_queue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_bytes, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_bytes\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# if we are at the end of the generation then we try again allowing for early token stopping\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_bytes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/code/ms/guidance/guidance/models/_grammarless.py:183\u001b[0m, in \u001b[0;36mGrammarlessEngine._start_generator_stream\u001b[0;34m(self, generator)\u001b[0m\n\u001b[1;32m    181\u001b[0m first_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m    184\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot chunk: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(chunk))\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/code/ms/guidance/guidance/models/_googleai.py:211\u001b[0m, in \u001b[0;36mGoogleAIChatEngine._start_generator\u001b[0;34m(self, system_text, messages, temperature)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;66;03m# append any image\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_parts):\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;66;03m# parts.append(Part.from_image(Image.from_bytes(self[raw_parts[i+1]])))\u001b[39;00m\n\u001b[1;32m    208\u001b[0m             parts\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    209\u001b[0m                 Part(\n\u001b[1;32m    210\u001b[0m                     inline_data\u001b[38;5;241m=\u001b[39mBlob(\n\u001b[0;32m--> 211\u001b[0m                         mime_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mraw_parts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 )\n\u001b[1;32m    214\u001b[0m             )\n\u001b[1;32m    215\u001b[0m     formated_messages\u001b[38;5;241m.\u001b[39mappend(Content(role\u001b[38;5;241m=\u001b[39mm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m], parts\u001b[38;5;241m=\u001b[39mparts))\n\u001b[1;32m    216\u001b[0m last_user_parts \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    217\u001b[0m     formated_messages\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    218\u001b[0m )  \u001b[38;5;66;03m# remove the last user stuff that goes in send_message (and not history)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'GoogleAIChatEngine' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with user():\n",
    "    lm += \"What is this? \" + image(triangle_img_path)\n",
    "\n",
    "with assistant():\n",
    "    lm += gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
